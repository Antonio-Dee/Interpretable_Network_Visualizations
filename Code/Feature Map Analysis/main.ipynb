{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI THESIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#\t\t\t\t\t\t\t#\n",
    "#         Main.py\t\t\t#\n",
    "#\t\t\t\t\t\t\t#\n",
    "#############################\n",
    "\n",
    "# Import needed libraries\n",
    "from dis import dis\n",
    "import os\n",
    "from sre_constants import GROUPREF_EXISTS\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Suppress warnings\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ' + tf.__version__)\n",
    "print('Keras version: ' + tf.keras.__version__)\n",
    "tf.get_logger().setLevel('ERROR') # Suppress warnings\n",
    "from buildModel import buildModelVGG16ExplicitTL, buildModelVGG16ExplicitFT\n",
    "from dataLoader import loadPreprocessedData\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.spatial import distance \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg16\n",
    "\n",
    "# Test Tensorflow version\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 432 #543\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "dataset_dir = 'imagenette2-320'\n",
    "model_dir = 'CNN'\n",
    "\n",
    "training_dir = os.path.join(dataset_dir, 'train')\n",
    "validation_dir = os.path.join(dataset_dir, 'val')\n",
    "test_dir = os.path.join(dataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(os.listdir(training_dir))\n",
    "\n",
    "print('Using dataset from folder: ' + dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#   VGG16 Network (Transfer Learning & Fine Tuning)   #\n",
    "#######################################################\n",
    "\n",
    "# Define model metadata\n",
    "input_shape = (256, 256, 3)\n",
    "classes = 10\n",
    "tl_epochs = 200\n",
    "ft_epochs = 200\n",
    "\n",
    "# Load dataset\n",
    "batch_size = 32\n",
    "train_val_gen = loadPreprocessedData(training_dir, validation_dir, test_dir, seed, batch_size, preprocess_vgg16)\n",
    "train_gen = train_val_gen['train']\n",
    "valid_gen = train_val_gen['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_train = 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_train == 'Y':\n",
    "### TRANSFER LEARNING ###\n",
    "\n",
    "# Create model\n",
    "        model = buildModelVGG16ExplicitTL(input_shape, classes, tfk, tfkl, seed)\n",
    "\n",
    "        # Create folders\n",
    "        exps_dir = os.path.join(model_dir)\n",
    "        if not os.path.exists(exps_dir): os.makedirs(exps_dir)\n",
    "        exp_dir = os.path.join(exps_dir, 'CNN_' + str(now))\n",
    "        if not os.path.exists(exp_dir): os.makedirs(exp_dir)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "                x = train_gen,\n",
    "                epochs = tl_epochs,\n",
    "                validation_data = valid_gen,\n",
    "                callbacks = [tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)]\n",
    "        ).history\n",
    "\n",
    "        # Save best epoch model\n",
    "        print()\n",
    "        model.save(model_dir + \"/\" + str(now) + '/model_tl')\n",
    "        np.save(model_dir + \"/\" + str(now) + \"/history_tl.npy\", history)\n",
    "        print()\n",
    "\n",
    "        ### FINE TUNING ###\n",
    "\n",
    "        # Create model\n",
    "        model = buildModelVGG16ExplicitFT(model, tfk)\n",
    "\n",
    "        # Create folders\n",
    "        exps_dir = os.path.join(model_dir)\n",
    "        if not os.path.exists(exps_dir): os.makedirs(exps_dir)\n",
    "        exp_dir = os.path.join(exps_dir, 'CNN_' + str(now))\n",
    "        if not os.path.exists(exp_dir): os.makedirs(exp_dir)\n",
    "\n",
    "        # Train the model\n",
    "        history_ft = model.fit(\n",
    "                x = train_gen,\n",
    "                epochs = ft_epochs,\n",
    "                validation_data = valid_gen,\n",
    "                callbacks = [tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)]\n",
    "        ).history\n",
    "\n",
    "        # Save best epoch model\n",
    "        print()\n",
    "        model.save(model_dir + \"/\" + str(now) + '/model_ft')\n",
    "        np.save(model_dir + \"/\" + str(now) + \"/history_ft.npy\", history_ft)\n",
    "        print()\n",
    "\n",
    "        # Plot the training history\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(history['loss'], label='Training TL',\n",
    "                alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "        plt.plot(history['val_loss'],\n",
    "                label='Validation TL', alpha=.8, color='#ff7f0e')\n",
    "        plt.plot(history_ft['loss'], label='Training FT',\n",
    "                alpha=.3, color='#8fce00', linestyle='--')\n",
    "        plt.plot(history_ft['val_loss'],\n",
    "                label='Validation FT', alpha=.8, color='#8fce00')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Categorical Crossentropy')\n",
    "        plt.grid(alpha=.3)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(history['accuracy'], label='Training TL',\n",
    "                alpha=.8, color='#ff7f0e', linestyle='--')\n",
    "        plt.plot(history['val_accuracy'],\n",
    "                label='Validation TL', alpha=.8, color='#ff7f0e')\n",
    "        plt.plot(history_ft['accuracy'], label='Training FT',\n",
    "                alpha=.8, color='#8fce00', linestyle='--')\n",
    "        plt.plot(history_ft['val_accuracy'],\n",
    "                label='Validation FT', alpha=.8, color='#8fce00')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Accuracy')\n",
    "        plt.grid(alpha=.3)\n",
    "        plt.show()\n",
    "\n",
    "        # Evaluation\n",
    "        predictions = model.predict(valid_gen)\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cmat = confusion_matrix(valid_gen.classes, np.argmax(predictions, axis=-1))\n",
    "\n",
    "        # Compute the classification metrics\n",
    "        accuracy = accuracy_score(valid_gen.classes, np.argmax(predictions, axis=-1))\n",
    "        precision = precision_score(valid_gen.classes, np.argmax(predictions, axis=-1), average='macro')\n",
    "        recall = recall_score(valid_gen.classes, np.argmax(predictions, axis=-1), average='macro')\n",
    "        f1 = f1_score(valid_gen.classes, np.argmax(predictions, axis=-1), average='macro')\n",
    "        print()\n",
    "        print('Validation Metrics:')\n",
    "        print('Accuracy:',accuracy.round(4))\n",
    "        print('Precision:',precision.round(4))\n",
    "        print('Recall:',recall.round(4))\n",
    "        print('F1:',f1.round(4))\n",
    "\n",
    "        #Plot the confusion matrix\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cmat.T, xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('True labels')\n",
    "        plt.ylabel('Predicted labels')\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "        model_path = input(\"Insert the path to the model folder you want to use: \")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Maps Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "valid_data_generator_np = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "valid_generator_np = valid_data_generator_np.flow_from_directory(directory=validation_dir,\n",
    "                                                target_size=(256,256),\n",
    "                                                color_mode='rgb',\n",
    "                                                classes=None, # can be set to labels\n",
    "                                                class_mode='categorical',\n",
    "                                                batch_size=1,\n",
    "                                                shuffle=True,\n",
    "                                                seed=seed)\n",
    "\n",
    "valid_data_generator_p = ImageDataGenerator(preprocessing_function=preprocess_vgg16)\n",
    "\n",
    "valid_generator_p = valid_data_generator_p.flow_from_directory(directory=validation_dir,\n",
    "                                                target_size=(256,256),\n",
    "                                                color_mode='rgb',\n",
    "                                                classes=None, # can be set to labels\n",
    "                                                class_mode='categorical',\n",
    "                                                batch_size=1,\n",
    "                                                shuffle=True,\n",
    "                                                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "valid_data_generator_np = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "valid_generator_np = valid_data_generator_np.flow_from_directory(directory=test_dir,\n",
    "                                                target_size=(256,256),\n",
    "                                                color_mode='rgb',\n",
    "                                                classes=None, # can be set to labels\n",
    "                                                class_mode='categorical',\n",
    "                                                batch_size=50,\n",
    "                                                shuffle=True,\n",
    "                                                seed=seed)\n",
    "\n",
    "valid_data_generator_p = ImageDataGenerator(preprocessing_function=preprocess_vgg16)\n",
    "\n",
    "valid_generator_p = valid_data_generator_p.flow_from_directory(directory=test_dir,\n",
    "                                                target_size=(256,256),\n",
    "                                                color_mode='rgb',\n",
    "                                                classes=None, # can be set to labels\n",
    "                                                class_mode='categorical',\n",
    "                                                batch_size=50,\n",
    "                                                shuffle=True,\n",
    "                                                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_np = next(valid_generator_np)\n",
    "batch_p = next(valid_generator_p)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch_p[0][i]\n",
    "original_image = batch_np[0][i]\n",
    "\n",
    "plt.imshow(original_image)\n",
    "plt.show()\n",
    "\n",
    "predicted_class = model.predict(image.reshape(1, 256, 256, 3))[0].tolist()\n",
    "\n",
    "max_value = max(predicted_class)\n",
    "mlc = predicted_class.index(max_value)\n",
    "print(\"Predicted Class: \" + labels[mlc])\n",
    "print(\"\\nConfidence\")\n",
    "confidence = list(map(lambda x, y: x+ ': ' +str(np.around(y*100, 3)) + '%', labels, predicted_class))\n",
    "for conf in confidence: print(conf)\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature maps from Grad-CAM + weights + Grad-CAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import skimage.filters\n",
    "from matplotlib import cm\n",
    "\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "conv_layers = [layer.output for layer in model.layers if isinstance(layer, tf.keras.layers.Conv2D)]  \n",
    "\n",
    "fmaps = []\n",
    "weights = []\n",
    "grad_cam = []\n",
    "\n",
    "for layer in conv_layers:\n",
    "\n",
    "   gradModel = tfk.models.Model(\n",
    "      inputs = [model.inputs],\n",
    "      outputs = [layer, model.output]\n",
    "   )\n",
    "\n",
    "   with tf.GradientTape() as tape:\n",
    "      inputs = tf.cast(tf.expand_dims(image, 0), tf.float32)\n",
    "      (convOutputs, predictions) = gradModel(inputs)\n",
    "      loss = predictions[..., 2]\n",
    "   \n",
    "   grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "   castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "   castGrads = tf.cast(grads > 0, \"float32\")\n",
    "   guidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "   \n",
    "   convOutputs = convOutputs[0]\n",
    "   guidedGrads = guidedGrads[0]\n",
    "\n",
    "   w = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "   cam = tf.reduce_sum(tf.multiply(w, convOutputs), axis=-1)\n",
    "\n",
    "   cam = tf.image.resize(np.expand_dims(cam, axis=2), [256, 256])\n",
    "\n",
    "   cam = np.divide(\n",
    "                np.subtract(cam, np.min(cam)), (np.max(cam)-np.min(cam))) if np.max(cam)-np.min(cam) != 0 else cam\n",
    "\n",
    "   fmaps.append(convOutputs)\n",
    "   weights.append(w)\n",
    "   grad_cam.append(cam)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "for g in grad_cam:\n",
    "\n",
    "   g_heatmap = (cm.jet(g))[:,:,0,0:3]\n",
    "   \n",
    "   g_heatmap = skimage.filters.gaussian(\n",
    "      g_heatmap, sigma=(1.0, 1.0), truncate=6.5, channel_axis=2)\n",
    "   g_heatmap = np.add(np.multiply(g_heatmap,0.5), np.multiply(original_image, 0.5))\n",
    "\n",
    "   plt.imshow(g_heatmap)\n",
    "   plt.show()\n",
    "   \n",
    "mean_gcam = np.max(grad_cam, axis=0)\n",
    "mean_gcam = np.divide(\n",
    "                np.subtract(mean_gcam, np.min(mean_gcam)), (np.max(mean_gcam)-np.min(mean_gcam))) if np.max(mean_gcam)-np.min(mean_gcam) != 0 else mean_gcam\n",
    "mean_gcam_heatmap = (cm.jet(mean_gcam))[:,:,0,0:3]\n",
    "mean_gcam_heatmap = skimage.filters.gaussian(\n",
    "         mean_gcam_heatmap, sigma=(1.0, 1.0), truncate=6.5, channel_axis=2)\n",
    "mean_gcam_overlay = np.add(np.multiply(mean_gcam_heatmap,0.5), np.multiply(original_image, 0.5))\n",
    "\n",
    "plt.imshow(mean_gcam_overlay)\n",
    "plt.show()\n",
    "\n",
    "model.layers[-1].activation = tf.keras.activations.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Maps Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from skimage.transform import resize\n",
    "from yellowbrick.cluster import silhouette_visualizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import skimage.filters\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_fmaps(fmaps):\n",
    "        n = fmaps.shape[-1]\n",
    "        filters = []\n",
    "        for i in range(0,n):\n",
    "                fmap = fmaps[:,:,i].flatten()\n",
    "                #norm = np.linalg.norm(fmap)\n",
    "                #if(norm != 0.0): fmap = fmap/norm\n",
    "                fmap = np.divide(\n",
    "                        np.subtract(fmap, np.min(fmap)), (np.max(fmap)-np.min(fmap))) if np.max(fmap)-np.min(fmap) != 0 else fmap\n",
    "                #p = 10\n",
    "                #fmap = 1/(1+np.e**(-p*(fmap-0.5)))\n",
    "                #fmap = (fmap - 1/(1+np.e**(-p*(0-0.5)))) / (1/(1+np.e**(-p*(1-0.5))) - 1/(1+np.e**(-p*(0-0.5)))) \n",
    "                filters.append(fmap)\n",
    "        \n",
    "        return filters\n",
    "\n",
    "def computeOverlay(heatmap, original_image, fmap_size):\n",
    "        #norm = np.linalg.norm(heatmap)\n",
    "        #if(norm != 0.0): heatmap = heatmap/norm\n",
    "        heatmap = np.divide(\n",
    "                np.subtract(heatmap, np.min(heatmap)), (np.max(heatmap)-np.min(heatmap))) if np.max(heatmap)-np.min(heatmap) != 0 else heatmap\n",
    "        #threshold = (np.max(heatmap) - np.min(heatmap))*0.33\n",
    "        #for i in range(len(heatmap)):\n",
    "        #        for j in range(len(heatmap[0])):\n",
    "        #                if heatmap[i][j] < threshold: heatmap[i][j] = 0.0\n",
    "\n",
    "        #heatmap = 0.5*np.cos(heatmap*np.pi+np.pi)+0.5\n",
    "        new_heatmap = skimage.filters.gaussian(\n",
    "                heatmap, sigma=(1.0, 1.0), truncate=5.5, channel_axis=2)\n",
    "        new_heatmap = np.divide(\n",
    "                np.subtract(new_heatmap, np.min(new_heatmap)), (np.max(new_heatmap)-np.min(new_heatmap))) if np.max(new_heatmap)-np.min(new_heatmap) != 0 else new_heatmap\n",
    "        jet_heatmap = (cm.jet(heatmap))[:,:,0:3]\n",
    "        overlay = np.add(np.multiply(jet_heatmap,0.5), np.multiply(original_image, 0.5))\n",
    "\n",
    "        return {'overlay':(255*overlay).astype(np.uint8), 'heatmap':resize(heatmap, (fmap_size, fmap_size))}\n",
    "\n",
    "def computeOverlayForMax(heatmap, original_image, fmap_size):\n",
    "        #norm = np.linalg.norm(heatmap)\n",
    "        #if(norm != 0.0): heatmap = heatmap/norm\n",
    "        #threshold = (np.max(heatmap) - np.min(heatmap))*0.33\n",
    "        #for i in range(len(heatmap)):\n",
    "        #        for j in range(len(heatmap[0])):\n",
    "        #                if heatmap[i][j] < threshold: heatmap[i][j] = 0.0\n",
    "\n",
    "        #heatmap = 0.5*np.cos(heatmap*np.pi+np.pi)+0.5\n",
    "        #heatmap = 1/(1+np.e**(-p*(heatmap-0.5)))\n",
    "        #heatmap = (heatmap - 1/(1+np.e**(-p*(0-0.5)))) / (1/(1+np.e**(-p*(1-0.5))) - 1/(1+np.e**(-p*(0-0.5)))) \n",
    "        new_heatmap = (cm.jet(heatmap))[:,:,0:3]\n",
    "        #new_heatmap = skimage.filters.gaussian(\n",
    "        #        new_heatmap, sigma=(1.0, 1.0), truncate=5.5, channel_axis=2)\n",
    "        overlay = np.add(np.multiply(new_heatmap,0.4), np.multiply(original_image, 0.6))\n",
    "\n",
    "        return {'overlay':(255*overlay).astype(np.uint8), 'heatmap':resize(heatmap, (fmap_size, fmap_size))}\n",
    "\n",
    "def compute_cluster_grid(groups, fmaps, cluster_no, original_image, indices, weights, if_print):\n",
    "        if not if_print: return\n",
    "        cluster = groups[cluster_no]\n",
    "        if len(cluster) == 0: return\n",
    "        row_size = 4\n",
    "        col_size = 4\n",
    "        total_dim = row_size*col_size\n",
    "        fmap_size = len(fmaps)\n",
    "        fmaps = tf.image.resize(fmaps, size=[256, 256]) # resize for visualization\n",
    "        original_image = tf.image.resize(original_image, size=[256, 256])\n",
    "        fig = plt.figure(figsize=(30, 30))\n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                        nrows_ncols=(row_size, col_size),  \n",
    "                        axes_pad=0.1,  # pad between axes in inch.\n",
    "                        )\n",
    "        n = 0\n",
    "        if len(cluster) < total_dim:\n",
    "                for i in range(len(cluster)):\n",
    "                        index = indices[cluster[i]]\n",
    "                        overlay = computeOverlay(fmaps[:, :, index], original_image, fmap_size)['overlay']\n",
    "                        grid[n].get_yaxis().set_ticks([])\n",
    "                        grid[n].get_xaxis().set_ticks([])\n",
    "                        grid[n].imshow(overlay, aspect='auto')\n",
    "                        #grid[n].imshow(fmaps[0, :, :, cluster[i]], cmap='viridis',aspect='auto')\n",
    "                        n += 1\n",
    "        else:\n",
    "                for i in range(total_dim):\n",
    "                        index = indices[cluster[i]]\n",
    "                        overlay = computeOverlay(fmaps[:, :, index], original_image, fmap_size)['overlay']\n",
    "                        grid[n].get_yaxis().set_ticks([])\n",
    "                        grid[n].get_xaxis().set_ticks([])\n",
    "                        grid[n].imshow(overlay, aspect='auto')\n",
    "                        #grid[n].imshow(fmaps[0, :, :, cluster[i]], cmap='viridis',aspect='auto')\n",
    "                        n += 1\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def compute_cluster_mean_median_max(groups, fmaps, cluster_no, original_image, w, indices, if_print, cluster_w, avg_cluster_w):\n",
    "        cluster = groups[cluster_no]\n",
    "        if len(cluster) == 0: return\n",
    "        row_size = 1\n",
    "        col_size = 3\n",
    "        fmap_size = len(fmaps)\n",
    "        fmaps = tf.image.resize(fmaps, size=[256, 256]) # resize for visualization\n",
    "        original_image = tf.image.resize(original_image, size=[256, 256])\n",
    "\n",
    "        mean_image = compute_cluster_mean(cluster, fmaps, original_image, w, indices, fmap_size)\n",
    "        median_image = compute_cluster_median(cluster, fmaps, original_image, indices, fmap_size)\n",
    "        max_image = compute_cluster_max(cluster, fmaps, original_image, indices, w, fmap_size)\n",
    "\n",
    "        if if_print:\n",
    "                fig = plt.figure(figsize=(30, 30))\n",
    "                grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                                nrows_ncols=(row_size, col_size),  \n",
    "                                axes_pad=0.1,  # pad between axes in inch.\n",
    "                                share_all=True\n",
    "                                )\n",
    "                grid[0].get_yaxis().set_ticks([])\n",
    "                grid[0].get_xaxis().set_ticks([])\n",
    "                grid[0].imshow(mean_image['overlay'], aspect='auto')\n",
    "                grid[1].imshow(median_image['overlay'], aspect='auto')\n",
    "                grid[2].imshow(max_image['overlay'], aspect='auto')\n",
    "                plt.show()\n",
    "\n",
    "        return {\n",
    "                'mean': mean_image['heatmap'], \n",
    "                #'median': median_image['heatmap'], \n",
    "                #'max': max_image['heatmap'], \n",
    "                'total_weight': cluster_w, \n",
    "                'average_weight': avg_cluster_w,\n",
    "                'image_count': len(groups[i])\n",
    "        }\n",
    "\n",
    "def compute_cluster_mean(cluster, fmaps, original_image, weights, indices, fmap_size):\n",
    "        mean = [[0]*256]*256\n",
    "        sumw = 0.0\n",
    "        for i in range(len(cluster)): \n",
    "                w = weights[cluster[i]]\n",
    "                index = indices[cluster[i]]\n",
    "                weightedprod = (w)*fmaps[:,:,index]\n",
    "                mean += weightedprod\n",
    "                sumw += w\n",
    "        mean = np.divide(mean, sumw)\n",
    "        overlay = computeOverlay(mean, original_image, fmap_size)\n",
    "        return overlay\n",
    "\n",
    "def compute_cluster_median(cluster, fmaps, original_image, indices, fmap_size):\n",
    "        filler = []\n",
    "        for i in range(len(cluster)): \n",
    "                map = fmaps[:,:,indices[cluster[i]]]\n",
    "                filler.append(map)\n",
    "        median = np.median(filler, axis = 0)\n",
    "        overlay = computeOverlay(median, original_image, fmap_size)\n",
    "        return overlay\n",
    "\n",
    "def compute_cluster_max(cluster, fmaps, original_image, indices, weights, fmap_size):\n",
    "        filler = []\n",
    "        for i in range(len(cluster)): \n",
    "                map = fmaps[:,:,indices[cluster[i]]]\n",
    "                w = weights[cluster[i]]\n",
    "                map = np.divide(\n",
    "                        np.subtract(map, np.min(map)), (np.max(map)-np.min(map))) if np.max(map)-np.min(map) != 0 else map\n",
    "                filler.append(map)\n",
    "        max = np.max(filler, axis=0)\n",
    "        overlay = computeOverlayForMax(max, original_image, fmap_size)\n",
    "        return overlay\n",
    "\n",
    "def compute_cluster_weight(cluster, weights):\n",
    "        sum = 0.0\n",
    "        for element in cluster:\n",
    "                sum += weights[element]\n",
    "        return np.round(sum, 3)\n",
    "\n",
    "def nullifyFeatureMaps(fmaps):\n",
    "        for m in range (0, len(fmaps)):\n",
    "                threshold = (np.max(fmaps[m]) - np.min(fmaps[m]))*0.33\n",
    "                for n in range(0, len(fmaps[m])):\n",
    "                        fmaps[m][n] = fmaps[m][n] if fmaps[m][n] >= threshold else 0.0\n",
    "        \n",
    "        return fmaps\n",
    "\n",
    "def get_data_radiant(data):\n",
    "        angle = np.arctan2(data[:, 1].max() - data[:, 1].min(), \n",
    "                        data[:, 0].max() - data[:, 0].min())\n",
    "        return np.clip(angle, np.radians(0.35), np.radians(0.55))\n",
    "\n",
    "def find_elbow(data, theta):\n",
    "\n",
    "    # make rotation matrix\n",
    "    co = np.cos(theta)\n",
    "    si = np.sin(theta)\n",
    "    rotation_matrix = np.array(((co, -si), (si, co)))\n",
    "\n",
    "    # rotate data vector\n",
    "    rotated_vector = data.dot(rotation_matrix)\n",
    "\n",
    "    # return index of elbow\n",
    "    return np.where(rotated_vector == rotated_vector.min())[0][0]\n",
    "\n",
    "def computeDiscardValue(n_fmaps, depth):\n",
    "        discard_value = 0\n",
    "\n",
    "        if n_fmaps >= 512:\n",
    "                discard_value = 0.175\n",
    "        elif n_fmaps >= 256:\n",
    "                discard_value = 0.35\n",
    "        elif n_fmaps >= 128:\n",
    "                discard_value = 0.7\n",
    "        elif n_fmaps >= 64:\n",
    "                discard_value = 1.4\n",
    "        else: \n",
    "                discard_value = 76.8/n_fmaps\n",
    "                \n",
    "        return discard_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_clustering = 'agglomerative_pruned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = 8\n",
    "if if_clustering == 'kmeans':\n",
    "        for depth in range(0,len(fmaps)):\n",
    "\n",
    "                print(\"Clustering at depth: \" + str(depth))\n",
    "\n",
    "                fmap_atdepth = fmaps[depth]\n",
    "                elements = np.array(reshape_fmaps(fmap_atdepth))\n",
    "                print(\"Number of feature maps: \" + str(len(elements)))\n",
    "\n",
    "                pca = PCA(n_components=64, random_state=43)\n",
    "                pca.fit(elements)\n",
    "                x = pca.transform(elements)\n",
    "\n",
    "                cluster_res = KMeansL1L2(n_clusters=nclusters, n_init = 100, max_iter= 300, tol=1e-6, random_state=None, norm='L2').fit(x)\n",
    "\n",
    "                centroids = cluster_res.cluster_centers_\n",
    "                n_labels = cluster_res.labels_\n",
    "\n",
    "                groups = [[] for i in range(len(centroids))]\n",
    "\n",
    "                min = 0\n",
    "\n",
    "\n",
    "                for i in range(0, n_labels.size): \n",
    "                        groups[n_labels[i]].append(i)\n",
    "\n",
    "                print(\"Number of clusters: \" + str(len(groups)))\n",
    "                print(centroids)\n",
    "                print()\n",
    "                for i in range(len(centroids)-1):\n",
    "                        for j in range(i+1,len(centroids)):\n",
    "                                print(\"Distance between \"+str(i+1)+\" and \"+str(j+1)+\": \"+str(distance.euclidean(centroids[i],centroids[j]))+\"\\n\")\n",
    "                for i in range(len(groups)): plot_cluster_grid(groups, fmap_atdepth, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_clustering == 'affinity':\n",
    "        for depth in range(0,len(fmaps)):\n",
    "\n",
    "                print(\"Clustering at depth: \" + str(depth))\n",
    "\n",
    "                fmap_atdepth = fmaps[depth]\n",
    "                elements = np.array(reshape_fmaps(fmap_atdepth))\n",
    "                print(\"Number of feature maps: \" + str(len(elements)))\n",
    "\n",
    "                pca = PCA(n_components=64, random_state=43)\n",
    "                pca.fit(elements)\n",
    "                x = pca.transform(elements)\n",
    "\n",
    "                cluster_res = AffinityPropagation(max_iter= 500, convergence_iter = 60, damping=0.7, random_state=5).fit(x)\n",
    "\n",
    "                centroids = cluster_res.cluster_centers_\n",
    "                n_labels = cluster_res.labels_\n",
    "\n",
    "                groups = [[] for i in range(len(centroids))]\n",
    "\n",
    "                min = 0\n",
    "\n",
    "\n",
    "                for i in range(0, n_labels.size): \n",
    "                        groups[n_labels[i]].append(i)\n",
    "\n",
    "                print(\"Number of clusters: \" + str(len(groups)))\n",
    "                print(centroids)\n",
    "                print()\n",
    "                for i in range(len(centroids)-1):\n",
    "                        for j in range(i+1,len(centroids)):\n",
    "                                print(\"Distance between \"+str(i+1)+\" and \"+str(j+1)+\": \"+str(distance.euclidean(centroids[i],centroids[j]))+\"\\n\")\n",
    "                for i in range(len(groups)): plot_cluster_grid(groups, fmap_atdepth, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_images = 1\n",
    "\n",
    "if if_clustering == 'dbscan':\n",
    "        for depth in range(0,len(fmaps)):\n",
    "\n",
    "                print('\\n')\n",
    "                print(\"=\" * 180)\n",
    "\n",
    "                print(\"Clustering at depth: \" + str(depth))\n",
    "\n",
    "                fmap_atdepth = fmaps[depth]\n",
    "                elements = np.array(reshape_fmaps(fmap_atdepth))\n",
    "                print(\"Number of feature maps: \" + str(len(elements)))\n",
    "                print(\"Size of feature map: \" + str(len(elements[0])))\n",
    "                \n",
    "                pca = PCA(n_components=0.8, random_state=43) #np.minimum(len(elements), len(elements[0]))\n",
    "                pca.fit(elements)\n",
    "                x = pca.transform(elements)\n",
    "                \n",
    "                neigh = NearestNeighbors(n_neighbors=2).fit(x)\n",
    "                distances, indexes = neigh.kneighbors(x)\n",
    "                distances = np.sort(distances, axis = 0)\n",
    "                distances=distances[:,1]\n",
    "                plt.plot(distances)\n",
    "                ind = np.arange(len(distances))\n",
    "                new_distances = np.vstack((ind, distances)).T\n",
    "                print(\"Angle: \" + str(np.rad2deg(get_data_radiant(new_distances))))\n",
    "                opt_eps = distances[find_elbow(new_distances, get_data_radiant(new_distances))]\n",
    "                # opt_eps = 0.3 + 0.1*np.log(depth+1)/np.log(25)\n",
    "                opt_eps = min(opt_eps, 0.5)\n",
    "                print(\"Optimal epsilon: \" + str(opt_eps))\n",
    "                plt.show()\n",
    "                \n",
    "                cluster_res = DBSCAN(eps = opt_eps, min_samples=2, metric='euclidean').fit(x)\n",
    "\n",
    "                n_labels = cluster_res.labels_\n",
    "                centroids = len(np.unique(n_labels))\n",
    "                \n",
    "                print(\"Number of clusters: \" + str(centroids-1) + \" + noisy cluster\")\n",
    "\n",
    "                groups = [[] for i in range(centroids)]\n",
    "                \n",
    "                for i in range(0, n_labels.size): \n",
    "                        groups[n_labels[i]+1].append(i)\n",
    "                \n",
    "                for i in range(1,len(groups)): \n",
    "                        if len(groups[i]) == 0: continue\n",
    "                        print(\"Cluster \" + str(i) + \":\")\n",
    "                        if print_all_images: plot_cluster_grid(groups, fmap_atdepth, i, original_image)\n",
    "                        else: plot_cluster_mean_median(groups, fmap_atdepth, i, original_image)\n",
    "\n",
    "                if print_all_images:    \n",
    "                        print(\"Noisy cluster :\")\n",
    "                        plot_cluster_grid(groups, fmap_atdepth, 0, original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_images = 1\n",
    "print_noisy = 1\n",
    "\n",
    "if if_clustering == 'optics':\n",
    "        for depth in range(0,len(fmaps)):\n",
    "\n",
    "                print('\\n')\n",
    "                print(\"=\" * 180)\n",
    "\n",
    "                print(\"Clustering at depth: \" + str(depth))\n",
    "\n",
    "                fmap_atdepth = np.asarray(fmaps[depth])\n",
    "                elements = np.array(reshape_fmaps(fmap_atdepth))\n",
    "                print(\"Number of feature maps: \" + str(len(elements)))\n",
    "                print(\"Size of feature map: \" + str(len(fmap_atdepth)) + 'x' + str(len(fmap_atdepth[0])))\n",
    "                \n",
    "                pca = PCA(n_components=0.8, random_state=43) #np.minimum(len(elements), len(elements[0]))\n",
    "                pca.fit(elements)\n",
    "                x = pca.transform(elements)\n",
    "                \n",
    "                cluster_res = OPTICS(min_samples = 5, metric=\"correlation\", cluster_method= \"xi\" , xi= 0, predecessor_correction=False).fit(x)\n",
    "                #cluster_res = OPTICS(min_samples = 3+int(len(elements)/512), p=1.5).fit(x)\n",
    "\n",
    "                n_labels = cluster_res.labels_\n",
    "                centroids = len(np.unique(n_labels))\n",
    "                \n",
    "                print(\"Number of clusters: \" + str(centroids-1) + \" + noisy cluster\")\n",
    "\n",
    "                groups = [[] for i in range(centroids)]\n",
    "\n",
    "                for i in range(0, n_labels.size):\n",
    "                        groups[n_labels[i]+1].append(i)\n",
    "                \n",
    "                for i in range(1,len(groups)): \n",
    "                        if len(groups[i]) == 0: continue\n",
    "                        print(\"Cluster \" + str(i) + \": \" + str(len(groups[i])) + \" images\")\n",
    "                        if print_all_images: plot_cluster_grid(groups, fmap_atdepth, i, original_image)\n",
    "                        else: plot_cluster_mean_median(groups, fmap_atdepth, i, original_image)\n",
    "\n",
    "                if (print_all_images*print_noisy):    \n",
    "                        print(\"Noisy cluster: \" + str(len(groups[0])) + \" images\")\n",
    "                        plot_cluster_grid(groups, fmap_atdepth, 0, original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_print = 1\n",
    "print_all_images = 0\n",
    "print_silhouette = 0\n",
    "starting_depth = len(fmaps)-1\n",
    "ending_depth = len(fmaps)-1\n",
    "\n",
    "from typing_extensions import final\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cluster_groups = []\n",
    "\n",
    "if if_clustering == 'agglomerative':\n",
    "        for depth in range(starting_depth,ending_depth+1):\n",
    "\n",
    "                print('\\n')\n",
    "                print(\"=\" * 180)\n",
    "\n",
    "                print(\"Clustering at depth: \" + str(depth))\n",
    "\n",
    "                fmap_atdepth = np.asarray(fmaps[depth])\n",
    "                weights_atdepth = weights[depth]\n",
    "\n",
    "                normalw = np.divide(weights_atdepth, sum(weights_atdepth))\n",
    "                normalw = np.multiply(normalw, 100)\n",
    "\n",
    "                elements = np.array(reshape_fmaps(fmap_atdepth))\n",
    "\n",
    "                newfmaps = []\n",
    "                newweights = []\n",
    "                newindices = []\n",
    "                for w in range(0, len(normalw)): \n",
    "                        discard = computeDiscardValue(len(elements), depth)\n",
    "                        if normalw[w] > discard:\n",
    "                                newfmaps.append(elements[w])\n",
    "                                newweights.append(normalw[w])\n",
    "                                newindices.append(w)\n",
    "                                \n",
    "                elements = newfmaps\n",
    "                #elements = nullifyFeatureMaps(elements)\n",
    "                print(\"Number of feature maps: \" + str(len(elements)) + \" (\" + str(len(normalw) - len(elements)) + \" images removed)\")\n",
    "                print(\"Size of feature map: \" + str(len(fmap_atdepth)) + 'x' + str(len(fmap_atdepth[0])))\n",
    "                \n",
    "                pca = PCA(n_components = 0.95, random_state=seed) #np.minimum(len(elements), len(elements[0]))\n",
    "                pca.fit(elements)\n",
    "                x = pca.transform(elements)\n",
    "\n",
    "                print(\"Dimensions for clustering: \" + str(len(x[0])))\n",
    "                \n",
    "                best_score = -np.inf\n",
    "                best_silhouette = -1\n",
    "                for n in range(3, 11): # 3-8 clusters\n",
    "                        cluster_res = AgglomerativeClustering(n_clusters = n).fit(x)\n",
    "                        silhouette_score = metrics.silhouette_score(x, cluster_res.labels_)\n",
    "                        \n",
    "                        if print_silhouette: silhouette_visualizer(cluster_res, x, colors='yellowbrick')\n",
    "                        \n",
    "                        n_labels_temp = cluster_res.labels_\n",
    "                        centroids_temp = len(np.unique(n_labels_temp))\n",
    "                        groups_temp = [[] for i in range(centroids_temp)]\n",
    "                        for i in range(0, n_labels_temp.size):\n",
    "                                groups_temp[n_labels_temp[i]].append(i)\n",
    "\n",
    "                        sum_size_distance = 0\n",
    "                        for i in range(0, len(groups_temp)):\n",
    "                                #print(len(groups_temp[i]))\n",
    "                                for j in range(0, len(groups_temp)):\n",
    "                                        sum_size_distance += (len(groups_temp[i]) - len(groups_temp[j]))**2 #distance squared\n",
    "                        avg_size_distance = (sum_size_distance/len(groups_temp)**2)\n",
    "\n",
    "                        score = np.longdouble((silhouette_score)**(int(len(normalw)/256) + 1))/(avg_size_distance)\n",
    "                        if(score > best_score and silhouette_score > best_silhouette*0.8): \n",
    "                                best_score = score\n",
    "                                if(silhouette_score > best_silhouette):\n",
    "                                        best_silhouette = silhouette_score\n",
    "                                n_labels = cluster_res.labels_\n",
    "\n",
    "                centroids = len(np.unique(n_labels))\n",
    "                \n",
    "                print(\"Number of clusters: \" + str(centroids))\n",
    "                print(\"Leftover weight: \" + str(np.sum(newweights)) + \"%\")\n",
    "\n",
    "                groups = [[] for i in range(centroids)]\n",
    "\n",
    "                for i in range(0, n_labels.size):\n",
    "                        groups[n_labels[i]].append(i)\n",
    "\n",
    "                cluster_images = []\n",
    "\n",
    "                for i in range(0,len(groups)): \n",
    "                        if len(groups[i]) == 0: continue\n",
    "                        print(\"Cluster \" + str(i+1) + \": \" + str(len(groups[i])) + \" images\")\n",
    "                        cluster_w = compute_cluster_weight(groups[i], newweights)\n",
    "                        print(\"Total weight (sum): \" + str(cluster_w) + \"%\")\n",
    "                        avg_cluster_w = np.round(cluster_w/len(groups[i]), 5)\n",
    "                        print(\"Average image weight: \" + str(avg_cluster_w) + \"%\")\n",
    "                        if print_all_images: compute_cluster_grid(groups, fmap_atdepth, i, original_image, newindices, if_print)\n",
    "                        else: \n",
    "                                dict_images = compute_cluster_mean_median_max(groups, fmap_atdepth, i, original_image, newweights, newindices, if_print, cluster_w, avg_cluster_w)\n",
    "                                cluster_images.append(dict_images)\n",
    "                \n",
    "                cluster_groups.append(cluster_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering with thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_print = 1\n",
    "print_all_images = 0\n",
    "print_silhouette = 0\n",
    "print_full_silhouette = 0\n",
    "print_threshold = 0\n",
    "starting_depth = 12\n",
    "ending_depth = len(fmaps)-1\n",
    "\n",
    "from typing_extensions import final\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cluster_groups = []\n",
    "\n",
    "if if_clustering == 'agglomerative_pruned':\n",
    "        for depth in range(starting_depth,ending_depth+1):\n",
    "\n",
    "                print('\\n')\n",
    "                print(\"=\" * 180)\n",
    "\n",
    "                print(\"Clustering at depth: \" + str(depth))\n",
    "\n",
    "                fmap_atdepth = np.asarray(fmaps[depth])\n",
    "                weights_atdepth = weights[depth]\n",
    "\n",
    "                normalw = np.divide(weights_atdepth, sum(weights_atdepth))\n",
    "                normalw = np.multiply(normalw, 100)\n",
    "\n",
    "                elements = np.array(reshape_fmaps(fmap_atdepth))\n",
    "\n",
    "                newfmaps = []\n",
    "                newweights = []\n",
    "                newindices = []\n",
    "                for w in range(0, len(normalw)): \n",
    "                        discard = computeDiscardValue(len(elements), depth)\n",
    "                        if normalw[w] > discard:\n",
    "                                newfmaps.append(elements[w])\n",
    "                                newweights.append(normalw[w])\n",
    "                                newindices.append(w)\n",
    "                                \n",
    "                elements = newfmaps\n",
    "                \n",
    "                #elements = nullifyFeatureMaps(elements)\n",
    "                print(\"Number of feature maps: \" + str(len(elements)) + \" (\" + str(len(normalw) - len(elements)) + \" images removed)\")\n",
    "                print(\"Size of feature map: \" + str(len(fmap_atdepth)) + 'x' + str(len(fmap_atdepth[0])))\n",
    "                \n",
    "                pca_n_components = min(50, min(len(elements), len(elements[0])))\n",
    "                pca = PCA(n_components=pca_n_components, random_state=seed) #np.minimum(len(elements), len(elements[0]))\n",
    "                x_pca = pca.fit_transform(elements)\n",
    "                tsne_perplexity = (2/3)*len(elements)\n",
    "                tsne = TSNE(n_components = 2, random_state=seed)\n",
    "                x = tsne.fit_transform(np.array(x_pca))\n",
    "\n",
    "                print(\"Dimensions for clustering: \" + str(len(x[0])))\n",
    "                \n",
    "                best_score = -np.inf\n",
    "                best_silhouette = -1\n",
    "                sil_x, sil_y = [],[]\n",
    "                for n in range(3, 9): # 3-8 clusters\n",
    "                        cluster_res = AgglomerativeClustering(n_clusters = n).fit(x)\n",
    "                        silhouette_score = metrics.silhouette_score(x, cluster_res.labels_)\n",
    "\n",
    "                        sil_x.append(n)\n",
    "                        sil_y.append(silhouette_score)\n",
    "                        \n",
    "                        score = silhouette_score\n",
    "\n",
    "                        #print(score)\n",
    "                        \n",
    "                        if(score > best_score and silhouette_score > best_silhouette*0.9): \n",
    "                                best_score = score\n",
    "                                if(silhouette_score > best_silhouette):\n",
    "                                        best_silhouette = silhouette_score\n",
    "                                n_labels = cluster_res.labels_\n",
    "\n",
    "                        if print_full_silhouette: silhouette_visualizer(cluster_res, x, colors='yellowbrick')\n",
    "\n",
    "                if print_silhouette:\n",
    "                        plt.plot(sil_x, sil_y)\n",
    "                        plt.xlabel('Number of Clusters')\n",
    "                        plt.ylabel('Silhouette Score')\n",
    "                        plt.title('Silhouette Scores with varying Cluster')\n",
    "                        plt.show()\n",
    "                centroids = len(np.unique(n_labels))\n",
    "                \n",
    "                print(\"Number of clusters: \" + str(centroids))\n",
    "                print(\"Leftover weight: \" + str(np.sum(newweights)) + \"%\")\n",
    "\n",
    "                groups = [[] for i in range(centroids)]\n",
    "                neurons_indices = [[] for i in range(centroids)]\n",
    "\n",
    "                for i in range(0, n_labels.size):\n",
    "                        groups[n_labels[i]].append(i)\n",
    "\n",
    "                cluster_images = []\n",
    "                cluster_w = []\n",
    "\n",
    "                for i in range(0,len(groups)): \n",
    "                        for n in range(0, len(groups[i])):\n",
    "                                neurons_indices[i].append(newindices[groups[i][n]])\n",
    "                        print(neurons_indices[i])\n",
    "                        if len(groups[i]) == 0:\n",
    "                                cluster_w.append(0)\n",
    "                                continue\n",
    "                        cluster_w.append(compute_cluster_weight(groups[i], newweights))\n",
    "                \n",
    "                threshold_w = max(max(cluster_w)/3, np.mean(cluster_w)/2)\n",
    "\n",
    "                print('Threshold:', str(threshold_w)+'%')\n",
    "                print()\n",
    "\n",
    "                if print_threshold:\n",
    "                        linspace = np.linspace(1, len(cluster_w), len(cluster_w))\n",
    "                        plt.scatter(linspace, cluster_w)\n",
    "                        plt.plot([linspace[0], linspace[len(linspace)-1]], [threshold_w, threshold_w], label='Threshold', c='r')\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                for i in range(len(groups)):\n",
    "                        deleted = '[removed]' if cluster_w[i] < threshold_w else ''\n",
    "                        print(\"Cluster \" + str(i+1) + \": \" + str(len(groups[i])) + \" images\", deleted)\n",
    "                        print(\"Total weight (sum): \" + str(cluster_w[i]) + \"%\")\n",
    "                        if cluster_w[i] < threshold_w: continue\n",
    "                        avg_cluster_w = np.round(cluster_w[i]/len(groups[i]), 5)\n",
    "                        if print_all_images: compute_cluster_grid(groups, fmap_atdepth, i, original_image, newindices, newweights, if_print)\n",
    "                        else: \n",
    "                                dict_images = compute_cluster_mean_median_max(groups, fmap_atdepth, i, original_image, newweights, newindices, if_print, cluster_w, avg_cluster_w)\n",
    "                                cluster_images.append(dict_images)\n",
    "                \n",
    "                cluster_groups.append(cluster_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Knowledge Collection Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing partial image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_save_partial = False\n",
    "if_save_final = False\n",
    "if_save_overlay = False\n",
    "partial_depth = 0\n",
    "partial_cluster = 2\n",
    "partial_type = 'mean'\n",
    "q = 0\n",
    "\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def computeBlur(val):\n",
    "        if val == 2:\n",
    "            return float(3.0)\n",
    "        if val == 4:\n",
    "            return float(6.0)\n",
    "        if val == 8:\n",
    "            return float(12.0)\n",
    "        if val == 16:\n",
    "            return float(16.0)\n",
    "        if val == 32:\n",
    "            return float(24.0)\n",
    "        return float(3.0)\n",
    "\n",
    "def apply_modifier(number):\n",
    "    return number*1.5+0.5\n",
    "\n",
    "def tile_array(a, b0, b1):\n",
    "        r, c = a.shape\n",
    "        rs, cs = a.strides\n",
    "        x = as_strided(a, (r, b0, c, b1), (rs, 0, cs, 0)) \n",
    "        return x.reshape(r*b0, c*b1) \n",
    "\n",
    "original_fmap = cluster_groups[partial_depth][partial_cluster][partial_type]\n",
    "\n",
    "#plt.imshow(original_fmap, cmap='jet')\n",
    "#plt.show()\n",
    "#plt.imshow(original_image)\n",
    "#plt.show()\n",
    "\n",
    "np_image = np.asarray(original_image)\n",
    "\n",
    "original_fmap = tile_array(original_fmap, int(len(np_image)/len(original_fmap)), int(len(np_image[0])/len(original_fmap[0])))\n",
    "\n",
    "total_len = len(original_image)*len(original_image[0])\n",
    "\n",
    "ordered_map = np.resize(original_fmap, total_len)\n",
    "user_input = ''\n",
    "\n",
    "number = 0\n",
    "while q < 100:\n",
    "    number+=1\n",
    "    q = apply_modifier(q)\n",
    "\n",
    "print(number)\n",
    "q = 0.5\n",
    "number = 0\n",
    "\n",
    "while not (user_input == 'q') and q <= 100:\n",
    "\n",
    "    percentile = np.percentile(ordered_map, q=100-q, axis=0)\n",
    "\n",
    "    print(percentile)\n",
    "\n",
    "    mask = tf.constant(original_fmap)\n",
    "    mask = tf.cast(mask >= percentile, \"float32\")\n",
    "\n",
    "    mask_trues = len(mask[mask == True])\n",
    "\n",
    "    blur_factor = computeBlur(int(512/len(original_fmap)))\n",
    "    mask = skimage.filters.gaussian(mask, sigma=(12, 12), truncate=1.5, channel_axis=-1)\n",
    "\n",
    "    print(mask_trues)\n",
    "    print(str(np.round(mask_trues*100/total_len, decimals = 3)) + '%')\n",
    "\n",
    "    modified_image = original_image\n",
    "    mask3d = np.stack((mask, mask, mask), axis=2)\n",
    "    modified_image = original_image*mask3d\n",
    "    modified_image = tf.image.resize(modified_image, size=[512, 512])\n",
    "\n",
    "    fig = plt.imshow(modified_image)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "    if if_save_partial and np.round(mask_trues*100/total_len, decimals = 3) < 95:\n",
    "        image_to_save = np.asarray(modified_image)\n",
    "        fname = \"img_test\"+str(number)+\".png\"\n",
    "        plt.imsave(fname, image_to_save)\n",
    "\n",
    "    q = apply_modifier(q)\n",
    "    number+=1\n",
    "    if not if_save_partial: user_input = input()\n",
    "\n",
    "plt.imshow(original_fmap, cmap='jet')\n",
    "plt.show()\n",
    "plt.imshow(original_image)\n",
    "plt.show()\n",
    "\n",
    "def apply_map(image, fmap):\n",
    "    blur_factor = computeBlur(int(512/len(fmap)))\n",
    "    fmap = skimage.filters.gaussian(fmap, sigma=(128, 128), truncate=6, channel_axis=-1)\n",
    "    fmap = np.divide(np.subtract(fmap, np.min(fmap)), (np.max(fmap)-np.min(fmap))) if np.max(fmap)-np.min(fmap) != 0 else fmap\n",
    "    fmap = (cm.jet(fmap))[:,:,0:3]\n",
    "\n",
    "    return np.add(np.multiply(fmap,0.4), np.multiply(image, 0.6))\n",
    "\n",
    "overlay_image = apply_map(original_image, original_fmap)\n",
    "plt.imshow(overlay_image)\n",
    "plt.show()\n",
    "if if_save_final:\n",
    "        img_to_save = tf.image.resize(original_image, size=[512, 512])\n",
    "        img_to_save = np.asarray(img_to_save)\n",
    "        plt.imsave(\"img_original.png\", img_to_save)\n",
    "\n",
    "if if_save_overlay:\n",
    "    overlay_image = tf.image.resize(overlay_image, size=[256,256])\n",
    "    overlay_image = np.asarray(overlay_image)\n",
    "    plt.imsave(\"img_overlay.png\", overlay_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7abd9f6edc37a3993f951030aabf2cbd5f261ae7f4bf9e85d2deb5c91c03c76d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
