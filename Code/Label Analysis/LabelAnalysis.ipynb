{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepl\n",
    "from keybert import KeyBERT\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import yellowbrick\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"something\")\n",
    "stop_words.add(\"like\")\n",
    "stop_words.add(\"multiple\")\n",
    "stop_words.add(\"lower\")\n",
    "stop_words.add(\"upper\")\n",
    "stop_words.add(\"typical\")\n",
    "stop_words.add(\"maybe\")\n",
    "stop_words.add(\"center\")\n",
    "stop_words.add(\"least\")\n",
    "stop_words.add(\"most\")\n",
    "stop_words.add(\"previous\")\n",
    "stop_words.add(\"next\")\n",
    "stop_words.add(\"left\")\n",
    "stop_words.add(\"right\")\n",
    "\n",
    "LABELS = ['Cassette Player', 'Chainsaw', 'Church', 'Dog', 'French Horn', 'Garbage Truck', 'Gas Pump', 'Golf Ball', 'Parachute', 'Fish']\n",
    "\n",
    "# Creating a new df\n",
    "df = pd.read_csv('db_for_data_analysis.csv')\n",
    "\n",
    "\n",
    "target_class = 'Parachute'\n",
    "target_class_it = 'Paracadute'\n",
    "target_label = LABELS.index(target_class) + 1\n",
    "data2 = df[df.language_en == 'f']\n",
    "data2 = data2[data2.guessed_label_id == target_label]['characteristic'].to_list()\n",
    "data = df[df.language_en == 't']\n",
    "data = data[data.guessed_label_id == target_label]['characteristic'].to_list()\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    translator = deepl.Translator(\"\")\n",
    "\n",
    "    result = translator.translate_text(data2, target_lang=\"EN-US\", source_lang=\"IT\", context=target_class)\n",
    "    \n",
    "    translation_dict = dict()\n",
    "    for i in range(len(data2)):\n",
    "        translation_dict.update({data2[i]: result[i].text})\n",
    "\n",
    "    translation_dict   \n",
    "    np.save(target_class.lower() + '_translation', translation_dict, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_dict = np.load(target_class.lower() + '_translation.npy', allow_pickle=True).item()\n",
    "translation_dict\n",
    "print(translation_dict.get(target_class_it.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in translation_dict.items():\n",
    "    translation_dict.update({k : ' ' + v.lower() + ' '})\n",
    "for e in range(len(data)):\n",
    "    data[e] = ' ' + data[e] + ' '\n",
    "data2 = data + list(translation_dict.values())\n",
    "original = [item.strip() for item in data2]\n",
    "\n",
    "for k, v in translation_dict.items():\n",
    "    translation_dict.update({k : v.strip()})\n",
    "\n",
    "data2 = [item.replace(' '+target_class.lower()+' ', ' ') for item in data2]\n",
    "if(translation_dict.get(target_class_it.lower()) is not None):\n",
    "    data2 = [item.replace(' '+translation_dict.get(target_class_it.lower())+' ', ' ') for item in data2]\n",
    "    data2 = [item.replace(' '+translation_dict.get(target_class_it.lower())+'\\'s ', ' ') for item in data2]\n",
    "data2 = [item.replace(' '+target_class.lower()+'\\'s ', ' ').strip() for item in data2]\n",
    "\n",
    "dictionary = {k: v for k, v in zip(original, data2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import nmslib\n",
    "\n",
    "# Creating the BERT embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "distinct_data = list(dict.fromkeys(data2))\n",
    "distinct_data.remove('')\n",
    "encoding_ = model.encode(distinct_data)\n",
    "data_encoding = np.array(encoding_)\n",
    "np.save('encoding.npy', data_encoding)\n",
    "embedding = np.load('encoding.npy')\n",
    "\n",
    "# Viewing the words embeddings\n",
    "vector_list = []\n",
    "for emb in embedding:\n",
    "    vector_list.append(emb)\n",
    "    \n",
    "vect = np.array(vector_list)\n",
    "\n",
    "new_df = pd.DataFrame({\"vector\": vector_list, \"title\": distinct_data})\n",
    "new_df = new_df.reset_index()\n",
    "new_df.drop(columns=[\"index\"], inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from yellowbrick.cluster import silhouette_visualizer\n",
    "from sklearn import metrics\n",
    "\n",
    "vectors = new_df['vector'].tolist()\n",
    "\n",
    "x, y = [], []\n",
    "for i in range(2, len(vectors)):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=i, affinity='cosine', linkage='complete')\n",
    "    labels = agglomerative.fit_predict(vectors)\n",
    "    sc = metrics.silhouette_score(vectors, labels)\n",
    "    x.append(i)\n",
    "    y.append(sc)\n",
    "    #print(agglomerative.distances_)\n",
    "    #print(agglomerative.children_)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[np.argmax(y)])\n",
    "agglomerative = AgglomerativeClustering(n_clusters=x[np.argmax(y)], affinity='cosine', linkage='complete')\n",
    "labels = agglomerative.fit_predict(vectors)\n",
    "\n",
    "new_df['cluster'] = labels\n",
    "new_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists of similar words\n",
    "syns = []\n",
    "from collections import defaultdict\n",
    "t = defaultdict(list)\n",
    "\n",
    "for g in set(new_df[\"cluster\"].to_list()):\n",
    "    rows = new_df[new_df[\"cluster\"] == g][\"title\"].to_list() # get all matching queries\n",
    "    syns.append(rows) # add list of synonyms to main list\n",
    "\n",
    "\n",
    "for i in range(len(syns)):\n",
    "    syns[i] = list(dict.fromkeys(syns[i]))\n",
    "    \n",
    "for l in range(len(syns)):\n",
    "    new_l = syns[l].copy()\n",
    "    for s in syns[l]:\n",
    "        count = data2.count(s)\n",
    "        for i in range(count - 1):\n",
    "            new_l.append(s)\n",
    "    syns[l] = new_l\n",
    "   \n",
    "merged_syns = syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    top_2 = []\n",
    "\n",
    "    for index in range(len(merged_syns)):\n",
    "        candidates = []\n",
    "        for sentence in merged_syns[index]:\n",
    "            words = sentence.split(' ')\n",
    "            w_product = list(product(words, words))\n",
    "            for pair in w_product:\n",
    "                if((pair[0] in stop_words) or (pair[1] in stop_words)):\n",
    "                    continue\n",
    "                if (pair[0] == pair[1]): \n",
    "                    if(pair[0] not in candidates): candidates.append(pair[0])\n",
    "                else: \n",
    "                    word = pair[0] + ' ' + pair[1]\n",
    "                    if(word not in candidates): candidates.append(word)\n",
    "        \n",
    "        candidates_vectors = model.encode(candidates) \n",
    "        sentences_vectors = model.encode(merged_syns[index]) \n",
    "        candidates_scores = []\n",
    "        for i in range(len(candidates)):\n",
    "            sum_dist = 0\n",
    "            for j in range(len(merged_syns[index])):\n",
    "                dist = cosine_similarity([candidates_vectors[i]], [sentences_vectors[j]])[0][0]\n",
    "                sum_dist += dist\n",
    "            candidates_scores.append(sum_dist)\n",
    "        \n",
    "        if(candidates == []):\n",
    "            top = ''\n",
    "        else:\n",
    "            top = candidates[np.argmax(candidates_scores)] \n",
    "        #print(top, ': ', merged_syns[index])\n",
    "        top_2.append(top)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    top_3 = []\n",
    "\n",
    "    for index in range(len(merged_syns)):\n",
    "        candidates = []\n",
    "        for sentence in merged_syns[index]:\n",
    "            words = sentence.split(' ')\n",
    "            w_product = list(product(words, words, words))\n",
    "            for pair in w_product:\n",
    "                if (pair[0] == pair[1] and pair[1] == pair[2]): \n",
    "                    if(pair[0] not in candidates): candidates.append(pair[0])           \n",
    "                elif (pair[0] == pair[1] and pair[1] != pair[2]):\n",
    "                    word = pair[1] + ' ' + pair[2]\n",
    "                    if(word not in candidates): candidates.append(word)\n",
    "                elif (pair[0] != pair[1] and pair[1] == pair[2]):\n",
    "                    word = pair[0] + ' ' + pair[1]\n",
    "                    if(word not in candidates): candidates.append(word)\n",
    "                elif (pair[0] != pair[1] and pair[0] == pair[2]):\n",
    "                    word = pair[0] + ' ' + pair[1]\n",
    "                    if(word not in candidates): candidates.append(word)  \n",
    "                else:\n",
    "                    word = pair[0] + ' ' + pair[1] + ' ' + pair[2]\n",
    "                    if(word not in candidates): candidates.append(word)\n",
    "        \n",
    "        candidates_vectors = model.encode(candidates) \n",
    "        sentences_vectors = model.encode(merged_syns[index]) \n",
    "        candidates_scores = []\n",
    "        for i in range(len(candidates)):\n",
    "            sum_dist = 0\n",
    "            for j in range(len(merged_syns[index])):\n",
    "                dist = cosine_similarity([candidates_vectors[i]], [sentences_vectors[j]])[0][0]\n",
    "                sum_dist += dist\n",
    "            candidates_scores.append(sum_dist)\n",
    "        \n",
    "        \n",
    "        top = candidates[np.argmax(candidates_scores)]     \n",
    "        #print(top, ': ', merged_syns[index])\n",
    "        top_3.append(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(True):\n",
    "    top_1 = []\n",
    "    for index in range(len(merged_syns)):\n",
    "        candidates = []\n",
    "        for sentence in merged_syns[index]:\n",
    "            words = sentence.split(' ')\n",
    "            for word in words:\n",
    "                if word in stop_words: continue\n",
    "                if(word not in candidates): candidates.append(word)\n",
    "        \n",
    "        candidates_vectors = model.encode(candidates) \n",
    "        sentences_vectors = model.encode(merged_syns[index]) \n",
    "        candidates_scores = []\n",
    "        for i in range(len(candidates)):\n",
    "            sum_dist = 0\n",
    "            for j in range(len(merged_syns[index])):\n",
    "                dist = cosine_similarity([candidates_vectors[i]], [sentences_vectors[j]])[0][0]\n",
    "                sum_dist += dist\n",
    "            candidates_scores.append(sum_dist)\n",
    " \n",
    "        candidates_scores_indexed = set(zip(list(np.linspace(0, len(candidates_scores)-1, len(candidates_scores)).astype(int)), candidates_scores))\n",
    "        candidates_scores_indexed = sorted(candidates_scores_indexed, key=lambda a : a[1], reverse=True)\n",
    "        if(candidates_scores != []):\n",
    "            top = candidates[candidates_scores_indexed[0][0]]\n",
    "        else:\n",
    "            top = ''\n",
    "        \n",
    "        top_1.append(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(merged_syns)):\n",
    "    #print(\"top_1:\", top_1[i], '\\t\\t', merged_syns[i])\n",
    "    print(\"top_1:\", top_1[i], '\\t\\t', merged_syns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lemmas\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def verb_to_wordnet(verb_tag):\n",
    "    if verb_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if verb_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if verb_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if verb_tag.startswith('R'):\n",
    "        return 'r'\n",
    "\n",
    "lems = []\n",
    "for i in range(len(top_1)):\n",
    "    cont_sent_tag = nltk.pos_tag([top_1[i]])\n",
    "    wordnet_verb = verb_to_wordnet(cont_sent_tag[0][1])\n",
    "    if wordnet_verb is not None:\n",
    "        lems.append(lem.lemmatize(top_1[i], wordnet_verb))\n",
    "    else:\n",
    "        lems.append(top_1[i])\n",
    "    #print(nltk.pos_tag([word]))\n",
    "\n",
    "print(len(lems))\n",
    "print(len(top_1))\n",
    "\n",
    "#print(lem.lemmatize(\"are\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary to store the most frequent word for each lemma\n",
    "most_frequent = {}\n",
    "\n",
    "# Count the frequency of each word for each lemma\n",
    "for lemma in set(lems):\n",
    "    words = [word for word, lem in zip(top_1, lems) if lem == lemma]\n",
    "    most_frequent[lemma] = max(words, key=words.count)\n",
    "\n",
    "# Create a dictionary where the key is a word from top_1 and the value is the most frequent word with the same lemma\n",
    "word_dict = {word: most_frequent[lem] for word, lem in zip(top_1, lems)}\n",
    "\n",
    "for i in range(len(top_1)):\n",
    "    top_1[i] = word_dict.get(top_1[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonims_dict = dict()\n",
    "for i in range(len(merged_syns)):\n",
    "    for j in range(len(merged_syns[i])):\n",
    "        originals = [key for key, val in dictionary.items() if val == merged_syns[i][j]]\n",
    "        for original_sentence in originals:\n",
    "            synonims_dict.update({original_sentence : top_1[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('db_for_data_analysis.csv')\n",
    "df = df[df.guessed_label_id == target_label]\n",
    "df_en = df[df.language_en == 't']\n",
    "df_it = df[df.language_en == 'f']\n",
    "\n",
    "df_it['characteristic_en'] = df_it['characteristic'].apply(str.lower).map(translation_dict)\n",
    "df_en['characteristic_en'] = df_en['characteristic'].apply(str.lower)\n",
    "\n",
    "df = pd.concat([df_it, df_en])\n",
    "df['synonym'] = df['characteristic_en'].apply(str.lower).map(synonims_dict)\n",
    "print(df['synonym'])\n",
    "df.to_csv(target_class.lower()+'_with_synonyms.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
